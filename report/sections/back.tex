\section{Background}
\label{sec:back}
% 2. background section about k nearest meighors by k-d trees.
%     Perhaps give and discuss the pseudocode of the recursive version of the python code.
%
% It has to be thorough in the sense that it provides the necessary understanding to
% the reader about all stages involved in the nearest neighbor by k-d tree, so all
% stages need to be explained. However, why  do you include "propagation" there?
% It is currently outside the scope of the project, right? You can use the material
% you read in related work or motivation, but there is no reason to describe
% "propagate_patches".
%
% A picture is worth 1000 words, so I would suggest you use either the python code
% or your own pseudocode. In particular the python code for tree traversal (that also does
% brute force) is relatively clean, I would include it. But make sure to give the high level
% explanation that helps understanding: for example explain the test that computes
% the (node) variables named "first" and "second", and say that "first" is always the node
% that is on the same side of the current node's median as the query. Similarly explain
% the "to-visit" condition for the "second" node -- why is it safe and why is it a
% conservative approximation (?)
%
% Tree construction looks reasonably nice in python pseudocode as well ...
% I do not think  there is reason to show the pseudocode for bruteforce --- that
% should be clear --- say that it is an n^2 algorithm, and you will explain it
% in your implementation section.




Computing KNN is widely applied due to its simplicity and excellent empirical performance, it has no training phase, it can handle binary as well as multiclass data, and while often applied for comparing two images, it faces the challenge of slow performance because both images are of image size. 
\\[2mm]
When comparing two images; image A and image B, the goal of KNN is to find the patches of image B that are most similar to each query patch of image A, according to a measured distance. The naive approach is using brute force, in which each patch of image B is compared with each patch of image A, if each image has m patches the complexity of the search will be $O(m^2)$. See \hyperref[sec:brute]{Brute Force}. Although brute force is a simple solution, there exists several alternative techniques for computing KNN. One commonly applied method is using a generalisation of the binary tree; the k-d tree. 
\\[2mm]
The k-d tree consists of a root, nodes and leaves; the root represents all patches, the nodes represent a partitioned segment of the patches, and the leaves collectively contain all patches spread out on $2^{h+1}$ leaves where h is the height of the tree excluding leaves.
\\[2mm]
Using a k-d tree has advances w.r.t search performance, thanks to the binary tree structure it yields a complexity of $O(m\ \lg\ m)$ \cite{logmatches}.
\\[2mm]
Algorithms \hyperref[alg:main]{1}, \hyperref[alg:tree]{2} and \hyperref[alg:traverse]{3} below represent a recursive implementation of the k-d tree construction and the tree traversal as high-level pseudo-code. 
\todo[color=green!40, inline, size=\small]{Cosmin: should I mention that the pseudo-code is based on the Python code and the authors?}
Algorithm \hyperref[alg:main]{1} is the \texttt{main} function in which dimensionality of images A and B are reduced using PCA\footnote{Given a collection of points in two or higher dimensional space a principal component analysis (PCA) can be created by first choosing the line that minimises the average squared distance from a point to the line, resulting in an Eigenvector, second choosing the line perpendicular to the first, resulting in a new Eigenvector, and repeating the process will result in orthogonal basis vectors. These vectors are called principal components, and several related procedures principal component analysis (PCA).}, a k-d tree is created of image B, and all patches of image A are iterated through, of which a call to \texttt{\textsc{Traverse}} yields the best neighbours for each patch in A. 
% \\[2mm]


% \\[2mm]
% Given the total number of patches m, a tree height excluding leaves h and level that represents each level of the tree from 0 to h, each node will have $\dfrac{m}{2^{level}}$ of the image patches, where $level$ is the current level of the node in the tree in which $level \leq height+1$, $height$ is the height of the tree excluding leaves and $m$ is the total number of patches in the image. 

% \todo[inline, size=\small]{}
% \todo[color=green!40, inline, size=\small]{}

\begin{algorithm}[H]
\caption{Main}\label{main}
\begin{algorithmic}[1]
\Procedure{Main}{k}
\State $imA \gets \textsc{PCA(\textit{imageA})}$
\State $imB \gets \textsc{PCA(\textit{imageB})}$
\State $tree \gets \textsc{BuildTree(imB, imB.1, 0, 0, \textsc{None}\text{)}}$
\State $neighbours \gets \textsc{None}$
\BState \emph{\text{\textbf{foreach} \textit{query} \textbf{in} \textit{imA} \textbf{do}}}:
	\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{0}, \textit{neighbours}, \textit{k})}$
\BState
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}


Algorithm \hyperref[alg:tree]{2} represents the construction of the k-d tree. The tree construction is divided into two statements; working with the nodes (lines 4-9) or working with the leaves (lines 20-21). This condition is measured by computing the height of the tree and checking the current level against the height. 
\\[2mm]
The patches are partitioned by finding the dimension with the widest spread and choosing the median value of that dimension. The nodes, therefore, contain a dimension and a median value. Line 5 uses the function GetSplitDimension to attain the dimension, lines 6-7 sort the indices and patches w.r.t the chosen dimension and lines 9-10 pick out the median index and value. Once done, the median and the dimension are ready to be stored, which essentially creates that particular node of the tree. Lines 13-19 compute the left and right indices for the next iteration and recursively call the BuildTree function again, in order to complete the creation of nodes in the tree. 
\\[2mm]
Last, line 21 sorts the leaves w.r.t the indices that were sorted and partitioned throughout the recursive node creation.


\begin{algorithm}[H]
\caption{Building the Tree}\label{tree}
\begin{algorithmic}[1]
\Procedure{BuildTree}{patches, indices, depth, index, tree}
\State $maxDepth \gets \textsc{ComputeMaxDepth(\textit{patches})} $
\BState \emph{}
\If {depth < maxDepth-1}
\State $dim \gets \textsc{GetSplitDimension(\textit{patches})} $
\State $indices \gets \textsc{SortByDim(}indices, dim\text{)} $
\State $points \gets patches[indices] $
\BState \emph{}
\State $medianIdx \gets \textsc{GetMedianIdx(\textit{indices})} $
\State $median \gets \textsc{GetMedian(\textit{points})} $
\BState \emph{}
\State $tree \gets (median, dim) $
\State $depth \gets depth \text{ + 1} $
\BState \emph{}
\State $leftIdx  \gets \textsc{AddToLeft(index)} $
\State $rightIdx \gets \textsc{AddToRight(index)} $
\BState \emph{}
\State $ \textsc{BuildTree(}patches[: medianIdx], indices, depth, leftIdx , tree\text{)} $
\State $ \textsc{BuildTree(}patches[medianIdx :], indices, depth, rightIdx, tree\text{)} $
\Else
\State $ leaves[index] \gets \textsc{SortLeavesByIndices(}patches, indices\text{)} $
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}


The traversal is presented in Algorithm 3 below. It deals with three different stages; reaching a leaf, which is the first and second node to visit and a check to visit the second leaf. The first stage on lines 2-4, reaching a leaf, is straightforward. We reach a leaf; therefore, we perform brute force with the query patch and the patches of image B in that current leaf. 
% \\[2mm]



% leaf
% left first
% right first
% check second leaf (backtrack)




\begin{algorithm}[H]
\caption{The Tree Traversal}\label{traverse}
\begin{algorithmic}[1]
\Procedure{Traverse}{tree, query, nodeIndex, neighbours, k}
\If {$\text{\textbf{IsLeaf}}(nodeIndex)$}
\State $neighbours \gets \textsc{BruteForce(}tree, query, nodeIndex, neighbours, k\text{)}$
\State \Return neighbours
\EndIf
\BState \emph{}
\State $dimens \gets tree[nodeIndex].1 $
\State $median \gets tree[nodeIndex].0 $
\State $queryV \gets  query[dimens]$
\BState \emph{}
\If {queryV <= median}
\State $first  \gets \textsc{GoLeft(}nodeIndex \text{)}$
\State $second \gets \textsc{GoRight(}nodeIndex\text{)}$
\Else
\State $first  \gets \textsc{GoRight(}nodeIndex\text{)}$
\State $second \gets \textsc{GoLeft(}nodeIndex \text{)}$
\EndIf
\BState \emph{}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{first}, \textit{neighbours}, \textit{k})}$
\State $worstNeighbour \gets \textsc{Last of } neighbours$
\BState \emph{}
\If {(median - queryV) < worstNeighbour}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{second}, \textit{neighbours}, \textit{k})}$
\EndIf
\BState \emph{}
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}

The third stage, lines 18-21, determine whether we should visit the second node by checking if the difference between the median and the query is less than the worst nearest neighbour. If this is the case; the second node will be visited in a recursive call of Traverse on line 21. The check is an estimate to assure that it makes sense to continue the traversal, because if the worst nearest neighbours is a better result in XXXX. \todo[inline, size=\small]{Argument the logic behind this.}
~~
\\[2mm]
The second stage on lines 6-17 determines which following nodes are the first and second, respectively. The first is always the node that is on the same side of the current node's median as the query. We might think of this as going left first on lines 10-12 and right first on lines 13-15. The dimension and median of the current node is extracted from the tree, lines 6-7, and the query value based on the current dimension of that node, line 8. Line 17 executes a recursive call of Traverse in which the first node is the next node to be visited.
\\[2mm]
The goal is to implement the solution using Futhark since Futhark is a pure functional data-parallel array language, meaning it is syntactically and conceptually similar to other functional languages; however, Futhark can compile code to run on the CPU, or it can optimise and parallelise the code to run on the GPU. It is a great advantage to write fully parallel code in a high-level functional language. The downside is that Futhark has its limits; such as only supporting regular arrays and not supporting recursion. With this in mind, the pseudo-code, demonstrated above, will need to be rewritten without recursion. See more in the sections \hyperref[sec:kdtree]{k-d Tree Construction} and \hyperref[sec:traversal]{Tree Traversal}.


\begin{itemize}
	\item approximate, PCA
	\item implement in Futhark, why and what is different
	\item 
\end{itemize}



% \\[2mm]
% While this report focuses on computing the exact KNN, it will still be an approximate KNN, due to the use of PCA to reduce dimensionality of the pixels in each image.

% Although the recursive implementation is simple it does not work in the programming language Futhark. XXXXX why ?  


% EMPIRICAL PERFORMANCE MODELS (EPMS)
% What are EPMs?
% Empirical performance models are regression models that characterize a given algorithm’s performance across problem instances and/or parameter settings. These models can predict the performance of algorithms on previously unseen input, including previously unseen problem instances and or previously untested parameter settings and are useful for analyzing of how an algorithm performs under different conditions, select promising configurations for a new problem instance, or surrogate benchmarks.




\subsection{Related Work}


\subsubsection{PatchMatch}
PatchMatch uses a randomised algorithm for quickly finding approximate nearest neighbour fields (ANNF) between image patches. 
\\[2mm]
While previous solutions use KD-Trees with dimensionality reduction, PatchMatch instead searches through a 2D space of possible patch offset. 
The initial step is choosing random patches followed by 4-5 iterations containing two processes: (1) propagation applying a statistic that can be used to examine the relation between two signals or data sets, known as coherence, (2) random search in the concentric neighbourhood to seek better matches by multiple scales of random offsets. The argument is that one random choice when assigning a patch is non-optimal, however applying a large field of random assignments is likely a good choice because it populates a larger domain. 
\\[2mm]
The gains are particularly performance enabling interactive image editing and sparse memory usage, resulting in runtime O(mM log M) and memory usage O(M), where m is the number of pixels and M the number of patches. The downside is that PatchMatch depends on similar images because it searches in nearby regions in few iterations, which in turn also affects its accuracy. 



\subsubsection{Coherency Sensitive Hashing}
Coherency Sensitive Hashing (CSH) is inspired by the techniques of Locality Sensitive Hashing (LSH) and PatchMatch. CSH uses a likewise hashing scheme of LSH and is built on two overall stages, indexing and searching. 
\\[2mm]
The indexing stage applies 2D Walsh-Hadamard kernels in which the projections of each patch in image A and B are initially computed onto the WH kernels. Subsequently the hash tables are created by the following. First, it takes a random line defined by a patch and divides it into bins of a constant width while shifting the division by a random offset. Second, the patch is projected onto the most significant 2D Walsh-Hadamard kernels. Last, a hash value is assigned, being the index of the bin it falls into. 
\\[2mm]
Applying hash tables has the benefit that similar patches are likely to be hashed into the same entry. 
\\[2mm]
The searching stage starts by initialising an arbitrary candidate map of ANNF. This is followed by iterations through each patch in image A where: (1) the nearest neighbour candidates of image B are found using the current ANNF and the current hash table, (2) the current ANNF mapping is updated with approximate distances. 
\\[2mm]
Selecting candidates follows the appearance-based techniques of LSH and as well as the coherence-based techniques of PatchMatch. Choosing among the candidates is done in an approximate manner where the WH kernels rejection scheme for pattern matching is applied beneficially. 
\\[2mm]
CSH has `46\%` better performance than PatchMatch and a higher level of accuracy with error rates that are `22\%` lower in comparison






