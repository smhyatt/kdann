\section{Background}
\label{sec:back}
% 2. background section about k nearest meighors by k-d trees.
%     Perhaps give and discuss the pseudocode of the recursive version of the python code.
%
% It has to be thorough in the sense that it provides the necessary understanding to
% the reader about all stages involved in the nearest neighbor by k-d tree, so all
% stages need to be explained. However, why  do you include "propagation" there?
% It is currently outside the scope of the project, right? You can use the material
% you read in related work or motivation, but there is no reason to describe
% "propagate_patches".
%
% A picture is worth 1000 words, so I would suggest you use either the python code
% or your own pseudocode. In particular the python code for tree traversal (that also does
% brute force) is relatively clean, I would include it. But make sure to give the high level
% explanation that helps understanding: for example explain the test that computes
% the (node) variables named "first" and "second", and say that "first" is always the node
% that is on the same side of the current node's median as the query. Similarly explain
% the "to-visit" condition for the "second" node -- why is it safe and why is it a
% conservative approximation (?)
%
% Tree construction looks reasonably nice in python pseudocode as well ...
% I do not think  there is reason to show the pseudocode for bruteforce --- that
% should be clear --- say that it is an n^2 algorithm, and you will explain it
% in your implementation section.


% - the algorithms as pseudo-code
% - why Futhark: supports regular parallelism - which is why it doesnt have recursion, 
% - note on PCA and going on with an approximate solution

% Computing KNN is widely applied due to its simplicity and excellent empirical performance, it has no training phase, it can handle binary as well as multiclass data, and while often applied for comparing two images, it faces the challenge of slow performance because both images are of image size. 

KNN is used find the K nearest neighbours in some dimensionality D, and it is widely applied due to its simplicity and excellent empirical performance, it has no training phase, it works well handling multi-dimensional datasets. 
We look at a D dimensional space, where we have a set of reference points of size n, and a set of queries of size m, and we aim to compute the nearest neighbours according to a measured distance for each query. 
% KNN, the problem is to find the nearest neighbours in some dimensionality D and k-d trees are good 
% O(m*n)
\\[2mm]
% When comparing two images; image A and image B, the goal of KNN is to find the patches of image B that are most similar to each query patch of image A, according to a measured distance. 
The naive approach is using brute force, in which each reference point is compared with each query, meaning the complexity of the search will be $O(m\times n)$. The brute force approach is explained further in section \ref{sec:brute}. Although brute force is a simple solution, there exists several alternative techniques for computing KNN. One commonly applied method is using a generalisation of the binary tree, known as the k-d tree. 
\\[2mm]
The k-d tree consists of a root, internal nodes and leaves. 
The reference points are fully partitioned, such that all the internal nodes on each level represent all reference points. Two nodes on the same level do not intersect in terms of the reference points they represent.
% This is a spatial partitioning of the tree in which all the nodes on the same level in the tree represent non-overlapping partitions of the reference points. Two nodes on the same level do not trivially intersect in terms of the reference points they represent. It's a full partitioning, all the nodes on each level represent all reference points. 
The leaves do not necessarily contain one reference point each, they may contain several reference points, and in our approach we apply brute force on the leaves.  
% The k-d tree consists of a root, internal nodes and leaves; the root represents all patches, the nodes represent a partitioned segment of the patches, and the leaves collectively contain all patches spread out on $2^{h+1}$ leaves where h is the height of the tree excluding leaves.
\\[2mm]
Using a k-d tree improves search performance, in principal the average work complexity is $O(m\ \log\ n)$ \cite{logmatches}. % (at least when we are working on reasonably small dimensionality, reference to section) 
%thanks to the binary tree structure it yields a complexity of $O(m\ \lg\ m)$ \cite{logmatches}.

\subsection{Pseudo-code for the Computing KNN using k-d Trees}

Algorithms \hyperref[alg:main]{1}, \hyperref[alg:tree]{2} and \hyperref[alg:traverse]{3} below represent an implementation of the k-d tree construction and the tree traversal as high-level pseudo-code. Both Algorithms \hyperref[alg:tree]{2} and \hyperref[alg:traverse]{3} use divide and conquer recursion. 
Algorithm \hyperref[alg:main]{1} is the \texttt{main} function in which a k-d tree is created of the reference points, and iterations through the queries call \texttt{\textsc{Traverse}} to find the best neighbours for query. 
% dimensionality of images A and B are reduced using PCA\footnote{Given a collection of points in two or higher dimensional space a principal component analysis (PCA) can be created by first choosing the line that minimises the average squared distance from a point to the line, resulting in an Eigenvector, second choosing the line perpendicular to the first, resulting in a new Eigenvector, and repeating the process will result in orthogonal basis vectors. These vectors are called principal components, and several related procedures principal component analysis (PCA).}, 
% a k-d tree is created of image B, and all patches of image A are iterated through, of which a call to \texttt{\textsc{Traverse}} yields the best neighbours for each patch in A. 
% \\[2mm]


% \\[2mm]
% Given the total number of patches m, a tree height excluding leaves h and level that represents each level of the tree from 0 to h, each node will have $\dfrac{m}{2^{level}}$ of the image patches, where $level$ is the current level of the node in the tree in which $level \leq height+1$, $height$ is the height of the tree excluding leaves and $m$ is the total number of patches in the image. 

% \todo[inline, size=\small]{}
% \todo[color=green!40, inline, size=\small]{}

\begin{algorithm}[H]
\caption{Main}\label{main}
\begin{algorithmic}[1]
\Procedure{Main}{k, queries, refs}
% \State $imA \gets \textsc{PCA(\textit{refs})}$
% \State $imB \gets \textsc{PCA(\textit{queries})}$
\State $tree \gets \textsc{BuildTree(}\textit{refs}, \textit{refs}.idxs, 0, 0, \textsc{None}\text{)}$
\State $neighbours \gets \textsc{None}$
\BState \emph{\text{\textbf{foreach} \textit{query} \textbf{in} \textit{queries} \textbf{do}}}:
	\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{0}, \textit{neighbours}, \textit{k})}$
\BState
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Construction the Tree}

Algorithm \hyperref[alg:tree]{2} represents the construction of the k-d tree. The tree construction is divided into two stages; one the processes the internal nodes (lines 3-18) and one the handles the leaves (lines 19-20). 
The condition is implemented by comparing the current level with the tree height, and if the two are equal, it means that we have reached the leaves. 
% This condition is measured by computing the height of the tree and checking the current level against the height. 
\\[2mm]
For a given node, at any given node level the reference points are partitioned by finding the dimension with the widest spread across the points represented by that node, and choosing the median value of that dimension. The left child of that node will represent the first half of the reference points in sorted order, and the right child of that node will represent the rest. 
% The reference points are partitioned by finding the dimension with the widest spread and choosing the median value of that dimension. 
The nodes, therefore, contain a dimension and a median value. Line 4 uses the function GetSplitDimension to find the dimension of widest spread, lines 5-6 sort the indices and patches w.r.t the chosen dimension and lines 8-9 pick out the median index and value. Once done, the median and the dimension are ready to be stored, which essentially creates that particular node of the tree. Lines 12-18 compute the indices of the left and right children of the current node for the next iteration, and recursively call the BuildTree function again, in order to complete the creation of nodes in the tree. 
\\[2mm]
Last, lines 20-21 sorts the leaves w.r.t the indices that were sorted and partitioned throughout the recursive node creation.


\begin{algorithm}[H]
\caption{Building the Tree}\label{tree}
\begin{algorithmic}[1]
\Procedure{BuildTree}{refs, indices, depth, index, tree}
% \State $maxDepth \gets \textsc{ComputeMaxDepth(\textit{patches})} $
\BState \emph{}
\If {\textsc{IsLeaf}(depth)}
\State $dim \gets \textsc{GetSplitDimension(\textit{refs})} $
\State $indices \gets \textsc{SortByDim(}indices, dim\text{)} $
\State $points \gets refs[indices] $
\BState \emph{}
\State $medianIdx \gets \textsc{Length(\textit{indices})}/2 $
\State $median \gets \textit{points}[medianIdx] $
\BState \emph{}
\State $tree[index] \gets (median, dim) $
\State $depth \gets depth \text{ + 1} $
\BState \emph{}
\State $leftIdx  \gets \textsc{GetLeftChild}(index) $
\State $rightIdx \gets \textsc{GetRightChild}(index) $
\BState \emph{}
\State $ \textsc{BuildTree(}points[: medianIdx], indices[: medianIdx], depth, leftIdx , tree\text{)} $
\State $ \textsc{BuildTree(}points[medianIdx :], indices[medianIdx :], depth, rightIdx, tree\text{)} $
\Else
\State $ leaves[index].refs \gets refs $
\State $ leaves[index].idxs \gets indices $
% \State $ leaves[index] \gets \textsc{SortLeavesByIndices(}patches, indices\text{)} $
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[H]
% \caption{Building the Tree}\label{tree}
% \begin{algorithmic}[1]
% \Procedure{BuildTree}{patches, indices, depth, index, tree}
% \State $maxDepth \gets \textsc{ComputeMaxDepth(\textit{patches})} $
% \BState \emph{}
% \If {depth < maxDepth-1}
% \State $dim \gets \textsc{GetSplitDimension(\textit{patches})} $
% \State $indices \gets \textsc{SortByDim(}indices, dim\text{)} $
% \State $points \gets patches[indices] $
% \BState \emph{}
% \State $medianIdx \gets \textsc{GetMedianIdx(\textit{indices})} $
% \State $median \gets \textsc{GetMedian(\textit{points})} $
% \BState \emph{}
% \State $tree \gets (median, dim) $
% \State $depth \gets depth \text{ + 1} $
% \BState \emph{}
% \State $leftIdx  \gets \textsc{AddToLeft(index)} $
% \State $rightIdx \gets \textsc{AddToRight(index)} $
% \BState \emph{}
% \State $ \textsc{BuildTree(}points[: medianIdx], indices, depth, leftIdx , tree\text{)} $
% \State $ \textsc{BuildTree(}points[medianIdx :], indices, depth, rightIdx, tree\text{)} $
% \Else
% \State $ leaves[index] \gets \textsc{SortLeavesByIndices(}patches, indices\text{)} $
% \EndIf
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}


\subsection{Tree Traversal}

The traversal is presented in Algorithm 3 below. Each query is going to traverse the k-d tree, and at each internal node in the tree, it is going to decide whether it has to visit a certain child by making a conservative test to whether the KNN can possibly be improved in the regions that that node represents. If a leaf is reached, a brute force is performed w.r.t the query and all the reference points represented in that leaf, this is shown in line 2-4. The rest of the code corresponds to the recursive traversal, lines 6-21. 
% The traversal is presented in Algorithm 3 below. It deals with three different stages; reaching a leaf, which is the first and second node to visit and a check to visit the second leaf. The first stage on lines 2-4, reaching a leaf, is straightforward. We reach a leaf; therefore, we perform brute force with the query patch and the patches of image B in that current leaf. 

% \\[2mm]
% leaf
% left first
% right first
% check second leaf (backtrack)

\begin{algorithm}[H]
\caption{The Tree Traversal}\label{traverse}
\begin{algorithmic}[1]
\Procedure{Traverse}{tree, query, nodeIndex, neighbours, k}
\If {$\text{\textbf{IsLeaf}}(nodeIndex)$}
\State $neighbours \gets \textsc{BruteForce(}tree, query, nodeIndex, neighbours, k\text{)}$
\State \Return neighbours
\EndIf
\BState \emph{}
\State $dimens \gets tree[nodeIndex].dims $
\State $median \gets tree[nodeIndex].meds $
\State $queryV \gets  query[dimens]$
\BState \emph{}
\If {queryV <= median}
\State $first  \gets \textsc{GoLeft(}nodeIndex \text{)}$
\State $second \gets \textsc{GoRight(}nodeIndex\text{)}$
\Else
\State $first  \gets \textsc{GoRight(}nodeIndex\text{)}$
\State $second \gets \textsc{GoLeft(}nodeIndex \text{)}$
\EndIf
\BState \emph{}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{first}, \textit{neighbours}, \textit{k})}$
\State $worstNeighbour \gets \textsc{Last of } neighbours$
\BState \emph{}
\If {(median - queryV) < worstNeighbour}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{second}, \textit{neighbours}, \textit{k})}$
\EndIf
\BState \emph{}
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}

% The code denotes by first and second, the child node that is on the same side at the median of the current node . 
The code denotes by first, the child of the current node that is on the same side as median w.r.t the query, similarly second denotes the child of the current node that is on the opposite side of the median. This can be observed between the computations on lines 10-15. 
\todo[color=green!40, inline, size=\small]{Ret så det lyder bedre}
If the query value of the current dimension is less than the median of the current node, then first is the left child, and right otherwise. 

If we reach this node, then since the query is on the same side as the current median as the node denoted first, then one recursive call is always going to be made, because it means that we can always reach a leaf that potentially has a neighbour better than the worst one found so far.

The recursive call that follows the second node is not necessarily made, since it is guarded by a conservative condition that tests whether the distance from the query to the current median is better than the worst found neighbour so far. If this condition does not hold, it is safe not to visit the left child, because it is ensured that you cannot find any better neighbours than the ones we currently have, which is visible in lines 20-21. 

\todo[color=green!40, inline, size=\small]{hertil}

% The third stage, lines 18-21, determine whether we should visit the second node by checking if the difference between the median and the query is less than the worst nearest neighbour. If this is the case; the second node will be visited in a recursive call of Traverse on line 21. The check is an estimate to assure that it makes sense to continue the traversal, because if the worst nearest neighbours is a better result in XXXX. \todo[inline, size=\small]{Argument the logic behind this.}
% ~~
% \\[2mm]
% The second stage on lines 6-17 determines which following nodes are the first and second, respectively. The first is always the node that is on the same side of the current node's median as the query. We might think of this as going left first on lines 10-12 and right first on lines 13-15. The dimension and median of the current node is extracted from the tree, lines 6-7, and the query value based on the current dimension of that node, line 8. Line 17 executes a recursive call of Traverse in which the first node is the next node to be visited.
% \\[2mm]
The goal is to implement the solution using Futhark since Futhark is a pure functional data-parallel array language, that works efficiently on the GPU. 
It is a great advantage to write fully parallel code in a high-level functional language. However, since Futhark utilises regular parallelism it does not support recursion. With this in mind, the pseudo-code, demonstrated above, will need to be rewritten without divide-and-conquer recursion, which is demonstrated in the sections \hyperref[sec:kdtree]{k-d Tree Construction} and \hyperref[sec:traversal]{Tree Traversal}.

% Futhark because it is a data-parallel language and simpler to write code in compared to CUDA, and to prototype various implementations. 
% Futhark because it is a data-parallel language that supports regular parallelism that's basically why we cannot use recursion. 
% The recursion in the k-d tree construction is a divide and conquer recursion approach. GPU's do not support dynamic parallelism, you cannot just spawn threads from a kernel. You cannot say 'I'm going to spawn another parallel things that computes this and another parallel thing that computes that' 


% \\[2mm]
% While this report focuses on computing the exact KNN, it will still be an approximate KNN, due to the use of PCA to reduce dimensionality of the pixels in each image.

% Although the recursive implementation is simple it does not work in the programming language Futhark. XXXXX why ?  


% EMPIRICAL PERFORMANCE MODELS (EPMS)
% What are EPMs?
% Empirical performance models are regression models that characterize a given algorithm’s performance across problem instances and/or parameter settings. These models can predict the performance of algorithms on previously unseen input, including previously unseen problem instances and or previously untested parameter settings and are useful for analyzing of how an algorithm performs under different conditions, select promising configurations for a new problem instance, or surrogate benchmarks.




\subsection{Related Work}


\subsubsection{PatchMatch}
PatchMatch uses a randomised algorithm for quickly finding approximate nearest neighbour fields (ANNF) between image patches. 
\\[2mm]
While previous solutions use KD-Trees with dimensionality reduction, PatchMatch instead searches through a 2D space of possible patch offset. 
The initial step is choosing random patches followed by 4-5 iterations containing two processes: (1) propagation applying a statistic that can be used to examine the relation between two signals or data sets, known as coherence, (2) random search in the concentric neighbourhood to seek better matches by multiple scales of random offsets. The argument is that one random choice when assigning a patch is non-optimal, however applying a large field of random assignments is likely a good choice because it populates a larger domain. 
\\[2mm]
The gains are particularly performance enabling interactive image editing and sparse memory usage, resulting in runtime O(mM log M) and memory usage O(M), where m is the number of pixels and M the number of patches. The downside is that PatchMatch depends on similar images because it searches in nearby regions in few iterations, which in turn also affects its accuracy. 



\subsubsection{Coherency Sensitive Hashing}
Coherency Sensitive Hashing (CSH) is inspired by the techniques of Locality Sensitive Hashing (LSH) and PatchMatch. CSH uses a likewise hashing scheme of LSH and is built on two overall stages, indexing and searching. 
\\[2mm]
The indexing stage applies 2D Walsh-Hadamard kernels in which the projections of each patch in image A and B are initially computed onto the WH kernels. Subsequently the hash tables are created by the following. First, it takes a random line defined by a patch and divides it into bins of a constant width while shifting the division by a random offset. Second, the patch is projected onto the most significant 2D Walsh-Hadamard kernels. Last, a hash value is assigned, being the index of the bin it falls into. 
\\[2mm]
Applying hash tables has the benefit that similar patches are likely to be hashed into the same entry. 
\\[2mm]
The searching stage starts by initialising an arbitrary candidate map of ANNF. This is followed by iterations through each patch in image A where: (1) the nearest neighbour candidates of image B are found using the current ANNF and the current hash table, (2) the current ANNF mapping is updated with approximate distances. 
\\[2mm]
Selecting candidates follows the appearance-based techniques of LSH and as well as the coherence-based techniques of PatchMatch. Choosing among the candidates is done in an approximate manner where the WH kernels rejection scheme for pattern matching is applied beneficially. 
\\[2mm]
CSH has `46\%` better performance than PatchMatch and a higher level of accuracy with error rates that are `22\%` lower in comparison






