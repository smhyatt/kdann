\section{Background}
\label{sec:back}
% 2. background section about k nearest meighors by k-d trees.
%     Perhaps give and discuss the pseudocode of the recursive version of the python code.

Computing K-Nearest Neighbours (KNN) is widely used due to its simplicity and excellent empirical performance. It has no training phase and it can handle binary as well as multiclass data. When comparing two images the challenge of KNN is finding the patches of say image B that are most similar to the query patch of say image A according to some measured distance. While there are many ways to compute KNN, one commonly applied method is using the \textit{k-d} tree. The \textit{k-d} tree is a binary tree and in this case we define each node to represent a sub-image of the patches in the image representation and a partitioning of that sub-image, meaning the root will represent the whole image.
\\[2mm]
Once the tree is created, all leaves will together contain the full partitioned image and each node will represent $\dfrac{m}{2^{level}}$ of the image patches, where $level$ is the current level of the node in the tree in which $level \leq height+1$, $height$ is the height of the tree excluding leaves and $m$ is the total number of patches in the image. 
\\[2mm]
The benefit of using a \textit{k-d} tree is the advances w.r.t. search performance. Thanks to the binary tree structure a KNN search for patches will have a complexity of $O(n\ \lg\ n)$. The naive approach of computing KNN is the brute force method, comparing each patch of image B with each patch of image A, resulting in a complexity of $O(n^2)$ \cite{logmatches}.
\\[2mm]
% explain propagation assisted method
Many approaches 
\cite{kdann} The argument for this approach is that by computing the exact KNN of the first query patch it is then possible to propagate .... and since the assumption is that the images are similar, it is likely that the former query patch will have nearby results as the current and the next. 


\begin{algorithm}
\caption{Main}\label{main}
\begin{algorithmic}[1]
\Procedure{Main}{k}
\State $imA \gets \textsc{PCA(\textit{imageA})}$
\State $imB \gets \textsc{PCA(\textit{imageB})}$
\State $tree \gets \textsc{BuildTree(imB, imB.1, 0, 0, \textsc{None}\text{)}}$
\State $neighbours \gets \textsc{None}$
\BState \emph{\text{\textbf{foreach} \textit{query} \textbf{in} \textit{imA} \textbf{do}}}:
	\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{0}, \textit{neighbours}, \textit{k})}$
\BState
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{The Tree Traversal}\label{traverse}
\begin{algorithmic}[1]
\Procedure{Traverse}{tree, query, nodeIndex, neighbours, k}
\If {$\text{\textbf{IsLeaf}}(nodeIndex)$}
\State $neighbours \gets \textsc{BruteForce(}tree, query, nodeIndex, neighbours, k\text{)}$
\State \Return neighbours
\EndIf
\BState \emph{}
\State $dimens \gets tree[nodeIndex].1 $
\State $median \gets tree[nodeIndex].0 $
\State $queryV \gets  query[dimens]$
\BState \emph{}
\If {queryV <= median}
\State $first  \gets \textsc{GoLeft(}nodeIndex \text{)}$
\State $second \gets \textsc{GoRight(}nodeIndex\text{)}$
\Else
\State $first  \gets \textsc{GoRight(}nodeIndex\text{)}$
\State $second \gets \textsc{GoLeft(}nodeIndex \text{)}$
\EndIf
\BState \emph{}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{first}, \textit{neighbours}, \textit{k})}$
\State $worstNeighbour \gets \textsc{Last of } neighbours$
\BState \emph{}
\If {(median - queryV) < worstNeighbour}
\State $neighbours \gets \textsc{Traverse(\textit{tree}, \textit{query}, \textit{second}, \textit{neighbours}, \textit{k})}$
\EndIf
\BState \emph{}
\Return neighbours
\EndProcedure
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Building the Tree}\label{tree}
\begin{algorithmic}[1]
\Procedure{BuildTree}{patches, indices, depth, index, tree}
\State $maxDepth \gets \textsc{ComputeMaxDepth(\textit{patches})} $
\BState \emph{}
\If {depth < maxDepth-1}
\State $dim \gets \textsc{GetSplitDimension(\textit{patches})} $
\State $indices \gets \textsc{SortByDim(}patches, dim\text{)} $
\State $points \gets patches[indices] $
\BState \emph{}
\State $medianIdx \gets \textsc{GetMedianIdx(\textit{indices})} $
\State $median \gets \textsc{GetMedian(\textit{points})} $
\BState \emph{}
\State $tree \gets (median, dim) $
\State $depth \gets depth \text{ + 1} $
\BState \emph{}
\State $leftIdx  \gets \textsc{AddToLeft(index)} $
\State $rightIdx \gets \textsc{AddToRight(index)} $
\BState \emph{}
\State $ \textsc{BuildTree(}patches[: medianIdx], indices, depth, leftIdx , tree\text{)} $
\State $ \textsc{BuildTree(}patches[medianIdx :], indices, depth, rightIdx, tree\text{)} $
\Else
\State $ leaves[index] \gets \textsc{SortLeavesByIndices(}patches, indices\text{)} $
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}


While this report focuses on computing the exact KNN, it will still be an approximate KNN, due to the use of PCA to reduce dimensionality of the pixels in each image. 

% EMPIRICAL PERFORMANCE MODELS (EPMS)
% What are EPMs?
% Empirical performance models are regression models that characterize a given algorithmâ€™s performance across problem instances and/or parameter settings. These models can predict the performance of algorithms on previously unseen input, including previously unseen problem instances and or previously untested parameter settings and are useful for analyzing of how an algorithm performs under different conditions, select promising configurations for a new problem instance, or surrogate benchmarks.




\subsection{Related Work}


\subsubsection{PatchMatch}
PatchMatch uses a randomised algorithm for quickly finding approximate nearest neighbour fields (ANNF) between image patches. 

While previous solutions use KD-Trees with dimensionality reduction, PatchMatch instead searches through a 2D space of possible patch offset. 
The initial step is choosing random patches followed by 4-5 iterations containing two processes: (1) propagation applying a statistic that can be used to examine the relation between two signals or data sets, known as coherence, (2) random search in the concentric neighbourhood to seek better matches by multiple scales of random offsets. The argument is that one random choice when assigning a patch is non-optimal, however applying a large field of random assignments is likely a good choice because it populates a larger domain. 

The gains are particularly performance enabling interactive image editing and sparse memory usage, resulting in runtime O(mM log M) and memory usage O(M), where m is the number of pixels and M the number of patches. The downside is that PatchMatch depends on similar images because it searches in nearby regions in few iterations, which in turn also affects its accuracy. 



\subsubsection{Coherency Sensitive Hashing}
Coherency Sensitive Hashing (CSH) is inspired by the techniques of Locality Sensitive Hashing (LSH) and PatchMatch. CSH uses a likewise hashing scheme of LSH and is built on two overall stages, indexing and searching. 

The indexing stage applies 2D Walsh-Hadamard kernels in which the projections of each patch in image A and B are initially computed onto the WH kernels. Subsequently the hash tables are created by the following. First, it takes a random line defined by a patch and divides it into bins of a constant width while shifting the division by a random offset. Second, the patch is projected onto the most significant 2D Walsh-Hadamard kernels. Last, a hash value is assigned, being the index of the bin it falls into. 

Applying hash tables has the benefit that similar patches are likely to be hashed into the same entry. 

The searching stage starts by initialising an arbitrary candidate map of ANNF. This is followed by iterations through each patch in image A where: (1) the nearest neighbour candidates of image B are found using the current ANNF and the current hash table, (2) the current ANNF mapping is updated with approximate distances. 

Selecting candidates follows the appearance-based techniques of LSH and as well as the coherence-based techniques of PatchMatch. Choosing among the candidates is done in an approximate manner where the WH kernels rejection scheme for pattern matching is applied beneficially. 

CSH has `46\%` better performance than PatchMatch and a higher level of accuracy with error rates that are `22\%` lower in comparison






