 1 Introduction
	- inspiration: the original paper
		- it has been inspired by paper xxx for computing axx it has good computations in computer vision, however in order to solve the problem we need all the pieces of knn for the kdtree
 		  it is non-trivial so therefore we decided to concentrate on that. 
 	- how has the project changed direction
	- why is this project useful: knn is core of many applications in various fields such as computer vision, ML
	- high level: that is computing knn with kd trees, internal node semantically corresponds to subset of , and how we reason about whether to visit the next node. 
 	- so what is a k-d tree: to construct - each internal node represents a range of the reference points, and I do not have to compare each query with each reference point. 
	- what does the project contain (concentrate on exact KNN by KD trees, the original has a nice motivation to compute the exact knn by kdtree efficiently)
	- we changed the Implementation uses a slightly different approach in which we use multiple points in each node, rather than just one point. 
	- discuss the difficulties: this reduces the performance to n log n, how efficient it is depends on dim and k. 

	- summarise the conclusion : mention so called dimensionality curse - octal trees cannot do high dim - kd tree can in principal, in practise 2 main problems that may affect: the dimensionality is too high then we might have to visit most reference point and similarly if k is too high and the distance to the worst neighbour becomes to big then we visit more points. before all that introduce that the leaves contain a set of points and we apply brute on the leaf level, because it is still fast when the number of points are small - in the hundred.  
	- how do I address dimensionality curse? - we propose an efficient way to look at higher dimensionality than the original approach. the distance to the current median to the distance of the worst neighbour - now we consider the upper and lower bounds of all dimensions. Improving temporal locality by sorting the leaves by accessing the same reference points in the same order. 
	- a summary of the results: for this dimensionality we obtain these amazing speed-ups and for these k's and how this affects the speed-up etc. - purpose is to demonstrate that these things you have just said are in fact true - give a magical feel. - over 5 I see the performance reduces drastically ... 
	- what are the downsides of the solution - significant overhead and uses contains much more data - so that is why experiments are important to show which ds and ks run best. 
	- how is the report organised

	1.1 Acknowledgement


2 Background
	- the algorithms as pseudo-code
	- why Futhark: supports regular parallelism - which is why it doesn't have recursion, 
	- note on PCA and going on with an approximate solution

	2.1 RelatedWork
		2.1.1 PatchMatch
		2.1.2 Coherency Sensitive Hashing


3 Implementation
	- for each have a difficulties / shortcomings section
	- include nice code of each - high level showing the map reduce constructions and such - just pseudocode (simplified version)
		- it should show the parallel constructs - okay to add sort there from tree construction bc. the text should state that we are using batched mergesort
		- it's just nice to see the loops and mapping constructions - it doesn't have to be your code, it can be the nice solution before we fixed the tree construct with sort where we split it up etc. 
			- which will help you, because then you can discuss based on the code like you did in the background. 
		- it doesn't have to work and it can be incomplete

	3.1 The Brute Force Version
		- describing which parts are parallel and which are not - and why

	3.2 k-d Tree Construction
		"the idea is that we are reducing an irregular problem to a regular problem in terms of parallelism, by means of padding" - such that all parallel operations have the same size such that we can do loop interchange, distribution and so on - to efficiently optimise parallelism. 
		- essential thing: padding technique that allows regular parallelism which is much more efficient since you write the code with regular parallelism and flattening - argue that the proposed solution uses at most 1 more 1 padded element per leaf and the number of points per leaf is typically in the hundreds so it accounts to less than 1% overhead, so its totally feasible, and this overhead allows for a solution that is built on top of regular parallelism - the parallel dimension has the same size over all the elements of the map and so on. 
		 at least one more - it accounts to less than 1% overhead - so it is totally feasible - it allows to build on top of regular parallelism.
		-
		- what is needed to compute so we get a more accurate result: lower bound and upper bound. 
		
		Difficulties: 
			- batch sorting is a detail - shortcomings in compiler - it was not possible to have the sorting inside the map
				- because of some shortcomings in the Futhark compiler, it wasn't able to efficiently exploit the regular parallelism if we put the map on top a mergesort - it didn't manage to interchange and so forth, and the same for radix sort.   
			- not fusing the reduce because then Futhark will fuse them inside. 
				- not fusing the reduces, because if we refused them, then Futhark is not going to also utilise the parallelism inside
		
		- explaining the sorting solution
		- reasoning about choices w.r.t sorting over partition
			- and why are there three inner maps in the for-loop (due to sorting)

	3.3 Tree Traversal
		- explaining the solution to use a stack 
		- showing a figure of the first traversal: first go to the leaf in which the query naturally belongs, there is a unique leaf to which it naturally belongs to 
		- showing (2-3) figures of the continuous traversal with a stack

		3.3.1 Representing the Stack as an Integer
			- including an equation that shows the bit arithmetic of setVisited

		3.3.2 Validating Whether to Look at the `second`
			- reasoning about the original median check
			- showing and reasoning about Cosmin's equation
			- comparing the two
				- benefits, trade-offs

	3.4 The Full Implementation
		- explain how things are put together - main function
		- reason about sorting the queries by the leaves - why is it faster? optimises locality of reference, temporal locality because by sorting the queries by the leaves the same leaves are accessed in order. 


4 Experimental Evaluation
	Q: how should the various data sizes, k's and dimension be demonstrated? table/plot

	always test with the best version (sorting and all dimensions)

	4.1 Sorting over Partition 
		- plot and reasoning of partition vs. sorting - including sorting the queries after the leaves
		- the impact of sorting doesn't necessarily depend on D and K  

	4.2 Including all Dimensions in the Traversal
		- plot and reasoning of traverse with one median check vs. traverse checking all medians
		- show the `visited` of both solutions as a histogram
	
	4.3 Brute Force versus the k-d Tree
		- plot and reasoning of brute force vs. the best k-d tree

	- Reasoning that an increased number of points will give a higher speed-up





5 Conclusion








Can't use all dimensionalities:  
say you have 4.000.000 points and dimensionality 32 - on a given path from the root to the leaf - the height of that path is 14, which means that more than half of the dimensions are not split, which means you loose accuracy. 

possibly good speedup when height of the tree is several times higher than the # dimensions

















